# Scientific feed parser
Keeping up to date with current research can be a struggle.
Usually I scope through arxiv, browse conferences or use Google Scholar alerts to keep an overview.

Browsing conferences requires clicking at each paper manually in order to access the abstract. At the same time you cannot easily filter for preferences.
Google Scholar alerts are quite powerful as they can be finely tuned. However, all notifications are send via mail, which are easily overlooked or forgotten. 
Hence, this repo wants to encourage the use of a rss like feed that is similar to an inbox but better structured and easier to work with. 
Using a dedicated feed for literature also helps in keeping different tasks separated (like general mail traffic from research).

Sadly this workflow is not supported out of the box, at least for major ML conferences and Google Scholar alerts.
NOTE: If you solely use arxiv you do not need any of this code. Arxiv offers a direct API, which can generate you suiting xml files for your desired keywords/topics (https://info.arxiv.org/help/api/user-manual.html).

We present a conference parser, which collects all accepted papers and gathers the most information like title, abstract and authors of desired conference and outputs it into an atom feed file.


To apply this workflow to google scholar we rely on the service https://kill-the-newsletter.com/, which lets us convert mails into atom feed files. 
You need to create an inbox on their page and use it to subscribe to your Google Scholar alert. 
Do not forget to store the provided xml link, as the feed information is aggregated there.
This means, as soon as a new mail is sent by Google Scholar, it is appended to the xml.
However, the format of the xml file is not ideal and contains little information.
To circumvent these issues we created an additional parser for this.
NOTE: The mail confirmation still needs to be handled manually (see section Google Scholar confirmation).


All in all, the idea of this repo is to build xml files to enable a feed based workflow. 
These files can be loaded into different feed readers like elfeed (which I recommend). 
If you decide to use elfeed I encourage you to try https://github.com/sp1ff/elfeed-score, which lets you rank your paper feed based on your preferences.

To make things easier, we created the files for the last 5 years of the supported conferences. They are available at [gdrive](https://drive.google.com/drive/folders/1uhqv4nrJ60-dT4V12kmqqVgEXW3SmozQ).

The idea of using elfeed in combination with arxiv was inspired by https://cundy.me/post/elfeed/. 
Also, https://github.com/CPR-RSS/CPR-RSS.github.io presented the idea of parsing conferences generally into a feed.


## Install
The installation was tested on python 3.10. To install the necessary libraries simply run:
```shell
pip install -r requirements.txt
```

## Usage
There are two main scripts in this repo:
- [parse_feed.py](parse_feed.py): Parse xml files generated by the combi of Google Scholar and https://kill-the-newsletter.com/ into feed friendly format.
- [parse_conference.py](parse_conference.py): Generate xml file for a specific conference. 
 
Currently, the supported conferences are CVPR, WACV, NIPS, ECCV and ICML. Parsing takes only a few seconds, excluding NIPS. 
NIPS enforces strict DDOS regulations, which mostly hinders the usage of async requests via aiohttp.

All created feeds are stored by default in the ```result_feeds``` folder.

### Examples
#### Parse Conference (```parse_conference.py```)
There are two flags ```-c``` for conference and ```-y``` for the desired year. To generate the feed for the CVPR 2023 execute the following:
```shell
parse_conference.py -c CVPR -y 2023
```

#### Parse Feed (```parse_feed.py```)
If you want to parse a single feed file there are 4 important flags:
- ```-s```: The path of the source file (xml feed file produces by kill the newsletter).
- ```-t```: The name of the target file, which is used in your feed reader later.
- ```-o```: Information if the source file is stored online or locally.
- ```-a``` should the result be appended to the existing target file or be overwritten

The following code illustrates the usage with an example:
```shell
parse_conference.py -s gs_feed.xml -t https://parsed_feed.xml -o True -a True
```
This works good for a single feed, but if you have multiple feeds (e.g., to keep the topics separated) it gets laborious.
Therefore, you can define a constant source to target file mapping in the [config.yaml](misc/config.yaml) file and enable it by setting the flag ```-u``` to ```True```. 
There is a dummy example in the config as well.
NOTE: Using the ```-u``` flag runs the script solely based on the config file and all other flags are ignored.

## Google Scholar confirmation
Open the xml file you received via the kill the newsletter link. 
In there you should find a substring similar to `http://scholar.google.com/scholar_alerts?update_op=confirm_alert&#x26;amp;hl=en&#x26;amp;email_for_op=6ti0vasfasttads%40kill-the-newsletter.com&#x26;amp;alert_id=Y_NB9sdffrAJ&#x22;`.

You need to reformat into a valid link. For me 
```python
str = "http://scholar.google.com/scholar_alerts?update_op=confirm_alert&#x26;amp;hl=en&#x26;amp;email_for_op=6ti0vasfasttads%40kill-the-newsletter.com&#x26;amp;alert_id=Y_NB9sdffrAJ&#x22;"
print(str.replace("amp;", "&").replace(";", "").replace("#", ""))
```
worked. Enter the url into your browser to execute the confirmation.

## API keys
As the Google Scholar alerts contains only partial information, we need to collect the rest ourselves. 
We cannot provide this functionality for every paper recommendation, however, it is straightforward for the largest publishers.
They provide API interfaces, which we utilize to gather the information. To utilize these APIs you need to get the respective API keys (see below).
After acquiring a key you can add it to the [config.yaml](misc/config.yaml).
The code does not depend on the keys. 
If they are not specified, the requests for extra information are simply skipped.

### Publishers
- springer/nature: (https://dev.springernature.com/)
- elsevier: (https://dev.elsevier.com/)
- ieee: Support is planned, however, they allow only a limited amount of calls.
- arxiv:  It is free to use and no specific key is needed. Thank you for arXiv for use of its open access interoperability.

